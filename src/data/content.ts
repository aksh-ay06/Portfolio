export const techStack = "C++, Java, JavaScript, TypeScript, Python, React.js, Next.js, Node.js, Express, Redux Toolkit, GraphQL, MongoDB, MySQL, Postgres, Docker, Kubernetes, Git, Machine Learning, Pytorch";

export const projects = [
  { name: "Collaborative Whiteboard", link: "https://github.com/aksh-ay06/excalidraw", desc: "Real-time collaborative drawing application with Next.js." },
  { name: "Food Ordering App", link: "https://github.com/aksh-ay06/swiggy-clone", desc: "A Swiggy clone built with React and Redux for optimized performance." },
  { name: "Threads Clone", link: "https://github.com/aksh-ay06/threads", desc: "A social media application built with Next.js and MongoDB." },
  { name: "Predicting Cancer Risk", link: "https://github.com/aksh-ay06/cancer-risk-prediction", desc: "Machine learning model combining health survey data and pollution metrics to predict cancer risk." }
];

export const blogs = [
  { name: "Understanding Machine Learning Bias", link: "/blogs/machine-learning-bias", desc: "Exploring the challenges of bias in machine learning models and strategies to mitigate them." },
  { name: "Optimizing React Applications", link: "/blogs/react-optimization", desc: "Best practices for improving performance and efficiency in React apps." },
  { name: "Introduction to Next.js for Full-Stack Development", link: "/blogs/nextjs-fullstack", desc: "A beginner-friendly guide to building full-stack applications with Next.js." }
];

export const blogData: { [key: string]: { title: string; content: string } } = {
  "machine-learning-bias": {
    "title": "Understanding Machine Learning Bias",
    "content": "Machine Learning (ML) has been a game-changer in almost every area of our lives—think about personalized recommendations on your favorite streaming platform, fraud detection in banking, or even automated resume screening tools. While these algorithms can be incredibly powerful, there’s an important conversation happening around bias in machine learning and the real harm it can cause if left unchecked.\n\nIn this post, we’ll take a down-to-earth look at what ML bias is, where it comes from, why it matters, and what we can do about it.\n\n## What Exactly Is Machine Learning Bias?\n\nAt a high level, ML bias happens when a model produces results that are unfairly skewed toward certain individuals or groups. Even if it’s not intentional, it can have serious consequences—like systematically denying loans to qualified applicants or misidentifying faces of certain ethnicities.\n\nBut why does bias creep in? Often, it’s because machine learning models mirror the data they’re trained on. If the data reflects historical inequalities or only represents a narrow slice of the population, the model will adopt those biases.\n\nKey insight: The machine doesn’t have an inherent sense of right or wrong; it just learns from the data we give it.\n\n## How Does Bias Sneak Into a Model?\n\n1. Data Collection:\n   - Let’s say you’re building a facial recognition model, but almost all your training photos feature people of a certain age, skin tone, or ethnicity. When that model encounters different faces, it might struggle.\n   - Or imagine a hiring tool that’s built using data from a company that historically favored candidates from specific backgrounds. The tool will likely carry on that trend.\n\n2. Measurement and Labeling:\n   - Sometimes data is labeled incorrectly—maybe because of human error or inconsistencies. If a spam detection system is fed mislabeled emails, it will learn incorrect patterns about what spam 'looks like.'\n\n3. Algorithm Choices:\n   - Some algorithms might naturally magnify small differences in the data, which ends up disproportionately impacting certain groups.\n   - The way we define success can also lead to bias. If we only optimize for accuracy without considering fairness, we might end up with a model that does really well for one group but fails others.\n\n4. Human Decisions:\n   - It’s easy to underestimate the subtle ways our own beliefs and assumptions can shape the modeling process. Which features do we choose? How do we clean the data? What metrics do we look at?\n   - Confirmation bias can play a role: we tend to notice and value information that aligns with what we already suspect.\n\n## Everyday Examples of Bias\n\n- Hiring Tools: A tool trained on a company’s past hiring decisions might favor candidates with similar backgrounds or education, overlooking other strong (but different-looking) resumes.\n- Facial Recognition: A system might have high accuracy for lighter-skinned faces but make more mistakes with darker-skinned faces if that latter group is underrepresented in the training data.\n- Medical Diagnosis: If clinical data mostly comes from certain demographics, a model might not perform as well for patients outside that data range.\n\n## Why Addressing Bias Matters\n\n- Fairness and Equity: We talk a lot about AI making life 'easier,' but if it only works well for a specific subset of people, it’s not truly inclusive.\n- Public Trust: If people realize AI systems are systematically biased, they’ll lose confidence in those tools—harming both individuals and the businesses that deploy them.\n- Legal and Ethical Considerations: With increasing regulations, organizations risk heavy penalties (not to mention PR nightmares) if their AI models are found to be discriminatory.\n- Better Performance: Counterintuitive as it may sound, a fairer model is often a more robust one. When you train on balanced, representative data, the algorithm is better equipped for real-world scenarios.\n\n## Tackling Machine Learning Bias: Practical Approaches\n\n1. Start With Representative Data:\n   - Make sure the data you use reflects the diversity of the population you serve. This might mean seeking out additional data sources or using data augmentation techniques to balance things out.\n\n2. Pick the Right Metrics:\n   - Don’t just look at accuracy. Consider fairness metrics, like whether different groups are being treated similarly or if error rates are consistent across demographics.\n\n3. Be Wary of Hidden Proxies:\n   - Even if you remove direct sensitive attributes like race or gender, other information (like ZIP codes or hobbies) can inadvertently reveal those attributes. It’s crucial to watch for features that could indirectly reinforce bias.\n\n4. Use Fairness-Centered Algorithms:\n   - There are techniques to modify the dataset before training (pre-processing), tweak the training itself (in-processing), or adjust the model’s outputs afterward (post-processing). Each step is an opportunity to reduce bias.\n\n5. Human Oversight:\n   - Ultimately, humans can catch things a machine won’t. Domain experts, ethicists, and diverse stakeholders should examine both the data and the model’s outputs. If something feels off, it probably is.\n\n6. Stay Transparent and Accountable:\n   - Let people know how your model works, what data it was trained on, and how you’re addressing possible biases. Transparency helps build trust and encourages community feedback.\n\n## Building a Culture of Responsibility\n\nAddressing bias isn’t just a one-time exercise—it’s an ongoing process. This demands a culture that values ethics alongside innovation. Some ways to cultivate that mindset:\n\n- Educate Everyone: Have open conversations about bias with your data scientists, developers, product managers, and executives.\n- Interdisciplinary Collaboration: Bring in sociologists, psychologists, and other experts to round out your understanding of how AI can affect people’s lives.\n- Continuous Monitoring: Even after a model is deployed, keep tabs on its performance and update it with new, more diverse data as your user base evolves.\n\n## Wrapping Up\n\nMachine learning bias isn’t some abstract, far-off concern. It has real-world implications: from the products we buy to the services we access, and even to the opportunities we’re given. The good news is that with awareness, careful planning, and consistent effort, we can make our AI systems more equitable and trustworthy.\n\n- Remember: Bias can creep in at any stage—data collection, model training, or even post-deployment.\n- Focus on fairness: Evaluate your models using more than just accuracy.\n- Stay vigilant: Your job isn’t done once the model is deployed. Keep monitoring and refining.\n\nWhen we actively work to understand and address bias, we’re taking a huge step toward ensuring that machine learning benefits everyone—not just a select few. And that’s really the promise and the power of AI: to make our lives easier, more interesting, and more connected, but without leaving anybody behind."
},
  "react-optimization": {
    title: "Optimizing React Applications",
    content: "Best practices for improving performance and efficiency in React apps."
  },
  "nextjs-fullstack": {
    title: "Introduction to Next.js for Full-Stack Development",
    content: "A beginner-friendly guide to building full-stack applications with Next.js."
  }
};
